{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa51cee",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Task 2: Honorific Style Change</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de97983",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94bde86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch evaluate transformers accelerate>=0.26.0 sentencepiece einops sacrebleu google.generativeai openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5468cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import google.generativeai as genai\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from dataclasses import dataclass\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer, \n",
    "    GenerationConfig, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from huggingface_hub import login\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72129821",
   "metadata": {},
   "source": [
    "## Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4dccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "OPEN_API_KEY = os.getenv(\"OPEN_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "MODELS = [\n",
    "    \"GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct\",\n",
    "    \"GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"sail/Sailor2-8B\",\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    \"google/gemma-2-9b-it\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8516c5",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95043582",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"JavaneseHonorifics/Unggah-Ungguh\", \"translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c15ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d8dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7536a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaca15f",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e627345",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "797d1f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f02f2d7a224911a789b2912ff3aed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 728, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/share/pkg.8/python3/3.10.12/install/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/scratch/2137582.1.ivcbuyin/ipykernel_3489196/4122142561.py\", line 5, in <module>\n",
      "    model1 = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/usr2/collab/mrfarhan/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/usr2/collab/mrfarhan/.local/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4256, in from_pretrained\n",
      "    model.generation_config = GenerationConfig.from_pretrained(\n",
      "  File \"/usr2/collab/mrfarhan/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 1102, in from_pretrained\n",
      "    config = cls.from_dict(config_dict, **kwargs)\n",
      "  File \"/usr2/collab/mrfarhan/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 1137, in from_dict\n",
      "    config = cls(**{**config_dict, **kwargs})\n",
      "  File \"/usr2/collab/mrfarhan/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 509, in __init__\n",
      "    self.validate(is_init=True)\n",
      "  File \"/usr2/collab/mrfarhan/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py\", line 789, in validate\n",
      "    logger.warning_once(\n",
      "  File \"/usr2/collab/mrfarhan/.local/lib/python3.10/site-packages/transformers/utils/logging.py\", line 328, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You have set `use_cache` to `False`, but cache_implementation is set to hybrid. cache_implementation will have no effect.'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    }
   ],
   "source": [
    "model_name_1 = MODELS[0] # \"GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct\"\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name_1)\n",
    "\n",
    "model1 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_1,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fb16c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2e0c7e1082486399e472bb53b1754e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_2 = MODELS[1] # \"GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct\"\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name_2)\n",
    "\n",
    "model2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_2,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b34259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77fe01dd4704f77b7409ae28045fa16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_3 = MODELS[2] # \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(model_name_3)\n",
    "\n",
    "model3 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_3,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d10611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a76f3f34be94943b888ab30cd657785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_4 = MODELS[3] # \"sail/Sailor2-8B\"\n",
    "\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(model_name_4)\n",
    "\n",
    "model4 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_4,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75701755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7ce6eac1bd428d9599942c4abcac11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_5 = MODELS[4] # \"google/gemma-2-2b-it\"\n",
    "\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(model_name_5)\n",
    "\n",
    "model5 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_5,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9bbdfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a10bae1d0848a8b14113880dcf4745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name_6 = MODELS[5] # \"google/gemma-2-9b-it\"\n",
    "\n",
    "tokenizer6 = AutoTokenizer.from_pretrained(model_name_6)\n",
    "\n",
    "model6 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_6,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=f\"./cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca82e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelOpenAI = OpenAI(api_key=OPEN_API_KEY)\n",
    "\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "ModelGemini = genai.GenerativeModel('gemini-1.5-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fbc7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class RateLimiter:\n",
    "    def __init__(self, min_delay_ms=1000):\n",
    "        self.min_delay_ms = min_delay_ms\n",
    "        self.last_call = datetime.now()\n",
    "    \n",
    "    def wait(self):\n",
    "        now = datetime.now()\n",
    "        elapsed_ms = (now - self.last_call).total_seconds() * 1000\n",
    "        \n",
    "        if elapsed_ms < self.min_delay_ms:\n",
    "            remaining_ms = self.min_delay_ms - elapsed_ms\n",
    "            while (datetime.now() - now).total_seconds() * 1000 < remaining_ms:\n",
    "                pass\n",
    "        \n",
    "        self.last_call = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11036f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "from sacrebleu.metrics import BLEU, CHRF\n",
    "\n",
    "@dataclass\n",
    "class TranslationResult:\n",
    "    predicted: List[str]\n",
    "    target: List[str]\n",
    "    bleu_score: float\n",
    "    chrf_score: float\n",
    "    groups: List[int]\n",
    "\n",
    "class JavaneseTranslationEvaluator:\n",
    "    def __init__(self, model, tokenizer=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bleu = BLEU()\n",
    "        self.chrf = CHRF(word_order=2)  # Using chrF++ (word_order=2)\n",
    "        \n",
    "    def _create_honorific_translation_prompt(self, text: str, source_level: int, target_level: int) -> str:\n",
    "        honorific_levels = {\n",
    "            0: \"Ngoko (informal, familiar conversations)\",\n",
    "            1: \"Ngoko Alus (respectful but informal)\",\n",
    "            2: \"Krama (polite but not overly formal)\",\n",
    "            3: \"Krama Alus (highly respectful)\"\n",
    "        }\n",
    "        \n",
    "        prompt = f\"\"\"Translate this Javanese sentence from {honorific_levels[source_level]} to {honorific_levels[target_level]}:\n",
    "Source: {text}\n",
    "Target: \"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def generate_translation(self, prompt: str) -> str:\n",
    "        torch.cuda.empty_cache()\n",
    "        rate_limiter = RateLimiter(min_delay_ms=1000)\n",
    "        try:\n",
    "            if self.model == ModelOpenAI:\n",
    "                response = ModelOpenAI.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=0.7,\n",
    "                    max_tokens=100,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "                translation = response.choices[0].message.content\n",
    "                torch.cuda.empty_cache()\n",
    "                rate_limiter.wait()\n",
    "            elif self.model == ModelGemini:\n",
    "                response = ModelGemini.generate_content(\n",
    "                    prompt,\n",
    "                    generation_config=genai.types.GenerationConfig(\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        top_k=50,\n",
    "                        max_output_tokens=100\n",
    "                    )\n",
    "                )\n",
    "                translation = response.text\n",
    "                torch.cuda.empty_cache()\n",
    "                rate_limiter.wait()\n",
    "            else:\n",
    "                generation_config = {\n",
    "                    'num_return_sequences': 1,\n",
    "                    'temperature': 0.7,\n",
    "                    'top_p': 0.9,\n",
    "                    'top_k': 50,\n",
    "                    'pad_token_id': self.tokenizer.pad_token_id,\n",
    "                    'do_sample': True,\n",
    "                    'early_stopping': True,\n",
    "                    'repetition_penalty': 1.2,\n",
    "                    'min_length': 10,\n",
    "                    'max_new_tokens': 100\n",
    "                }\n",
    "                input_ids = self.tokenizer(prompt, return_tensors='pt').input_ids\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    **generation_config\n",
    "                )\n",
    "                translation = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            translation = translation.split(\":\")[-1].strip()\n",
    "            torch.cuda.empty_cache()\n",
    "            return translation if translation else \"EMPTY_TRANSLATION\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in generation: {str(e)}\")\n",
    "            return \"EMPTY_TRANSLATION\"\n",
    "            \n",
    "    def _calculate_scores(self, predictions: List[str], targets: List[str]) -> Tuple[float, float]:\n",
    "        \"\"\"Calculate BLEU and chrF++ scores with error handling.\"\"\"\n",
    "        try:\n",
    "            valid_pairs = [(p, t) for p, t in zip(predictions, targets) \n",
    "                          if p != \"EMPTY_TRANSLATION\" and p and t]\n",
    "            \n",
    "            if not valid_pairs:\n",
    "                return 0.0, 0.0\n",
    "                \n",
    "            valid_preds, valid_targets = zip(*valid_pairs)\n",
    "            \n",
    "            bleu_score = self.bleu.corpus_score(\n",
    "                valid_preds, \n",
    "                [valid_targets]\n",
    "            ).score\n",
    "            \n",
    "            chrf_score = self.chrf.corpus_score(\n",
    "                valid_preds,\n",
    "                [valid_targets]\n",
    "            ).score\n",
    "            \n",
    "            return bleu_score, chrf_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating scores: {str(e)}\")\n",
    "            return 0.0, 0.0\n",
    "            \n",
    "    def evaluate_translations(self, dataset: List[Dict]) -> Dict[str, TranslationResult]:\n",
    "        # Initialize results dictionary for all possible honorific level combinations\n",
    "        results = {}\n",
    "        for source_level in range(4):\n",
    "            for target_level in range(4):\n",
    "                if source_level != target_level:\n",
    "                    key = f'java_{source_level}_to_{target_level}'\n",
    "                    results[key] = []\n",
    "        \n",
    "        targets = {k: [] for k in results.keys()}\n",
    "        groups = {k: [] for k in results.keys()}\n",
    "\n",
    "        grouped_data = {}\n",
    "        for item in dataset:\n",
    "            if 'group' not in item or 'sentence' not in item or item['sentence'] is None:\n",
    "                print(f\"Skipping invalid data item: {item}\")\n",
    "                continue\n",
    "                \n",
    "            group = item['group']\n",
    "            if group not in grouped_data:\n",
    "                grouped_data[group] = {'translations': {}}\n",
    "\n",
    "            if item['sentence'] is not None and item['sentence'].strip():\n",
    "                grouped_data[group]['translations'][item['label']] = item['sentence']\n",
    "\n",
    "        for group, data in grouped_data.items():\n",
    "            for source_level in range(4):\n",
    "                if source_level not in data['translations']:\n",
    "                    continue\n",
    "                    \n",
    "                source_text = data['translations'][source_level]\n",
    "\n",
    "                for target_level in range(4):\n",
    "                    if source_level == target_level:\n",
    "                        continue\n",
    "                        \n",
    "                    if target_level in data['translations']:\n",
    "                        prompt = self._create_honorific_translation_prompt(\n",
    "                            source_text, source_level, target_level)\n",
    "                        prediction = self.generate_translation(prompt)\n",
    "                        \n",
    "                        key = f'java_{source_level}_to_{target_level}'\n",
    "                        results[key].append(prediction)\n",
    "                        targets[key].append(data['translations'][target_level])\n",
    "                        groups[key].append(group)\n",
    "\n",
    "        final_results = {}\n",
    "        for key in results:\n",
    "            if results[key] and all(t is not None for t in targets[key]):\n",
    "                bleu_score, chrf_score = self._calculate_scores(results[key], targets[key])\n",
    "                \n",
    "                final_results[key] = TranslationResult(\n",
    "                    predicted=results[key],\n",
    "                    target=targets[key],\n",
    "                    bleu_score=bleu_score,\n",
    "                    chrf_score=chrf_score,\n",
    "                    groups=groups[key]\n",
    "                )\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "def print_evaluation_results(results: Dict[str, TranslationResult]):\n",
    "    honorific_levels = {\n",
    "        '0': 'Ngoko',\n",
    "        '1': 'Ngoko Alus',\n",
    "        '2': 'Krama',\n",
    "        '3': 'Krama Alus'\n",
    "    }\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for key, result in results.items():\n",
    "        source_level = key.split('_')[1]\n",
    "        target_level = key.split('_')[-1]\n",
    "        \n",
    "        print(f\"\\n{honorific_levels[source_level]} to {honorific_levels[target_level]}:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"BLEU Score: {result.bleu_score:.2f}\")\n",
    "        print(f\"chrF++ Score: {result.chrf_score:.2f}\")\n",
    "\n",
    "def calculate_average_scores(results: Dict[str, TranslationResult]) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Calculate average BLEU and chrF++ scores for each source level and overall.\"\"\"\n",
    "    \n",
    "    scores = {\n",
    "        'overall': {'bleu': [], 'chrf': []}\n",
    "    }\n",
    "    \n",
    "    # Initialize scores for each source level\n",
    "    for i in range(4):\n",
    "        scores[f'from_level_{i}'] = {'bleu': [], 'chrf': []}\n",
    "    \n",
    "    for key, result in results.items():\n",
    "        source_level = int(key.split('_')[1])\n",
    "\n",
    "        if result.bleu_score > 0 or result.chrf_score > 0:\n",
    "            scores[f'from_level_{source_level}']['bleu'].append(result.bleu_score)\n",
    "            scores[f'from_level_{source_level}']['chrf'].append(result.chrf_score)\n",
    "            scores['overall']['bleu'].append(result.bleu_score)\n",
    "            scores['overall']['chrf'].append(result.chrf_score)\n",
    "\n",
    "    averages = {}\n",
    "    for direction in scores:\n",
    "        averages[direction] = {\n",
    "            'bleu': np.mean(scores[direction]['bleu']) if scores[direction]['bleu'] else 0.0,\n",
    "            'chrf': np.mean(scores[direction]['chrf']) if scores[direction]['chrf'] else 0.0\n",
    "        }\n",
    "    \n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f2d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Model2...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "\n",
    "def evaluate_all_models(models_dict: Dict, data: List[Dict]) -> pd.DataFrame:\n",
    "    # Initialize empty lists to store results\n",
    "    results_list = []\n",
    "    \n",
    "    # Generate all possible translation directions between different honorific levels\n",
    "    directions = []\n",
    "    for source_level in range(4):\n",
    "        for target_level in range(4):\n",
    "            if source_level != target_level:\n",
    "                directions.append(f'java_{source_level}_to_{target_level}')\n",
    "    \n",
    "    for model_name, (model, tokenizer) in models_dict.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        evaluator = JavaneseTranslationEvaluator(model, tokenizer)\n",
    "        results = evaluator.evaluate_translations(data)\n",
    "\n",
    "        row_data = {'Model': model_name}\n",
    "\n",
    "        for direction in directions:\n",
    "            if direction in results:\n",
    "                row_data[f'{direction}_BLEU'] = results[direction].bleu_score\n",
    "                row_data[f'{direction}_chrF++'] = results[direction].chrf_score\n",
    "            else:\n",
    "                row_data[f'{direction}_BLEU'] = 0.0\n",
    "                row_data[f'{direction}_chrF++'] = 0.0\n",
    "\n",
    "        for source_level in range(4):\n",
    "            source_directions = [d for d in directions if f'java_{source_level}_to_' in d]\n",
    "            bleu_scores = [row_data[f'{d}_BLEU'] for d in source_directions]\n",
    "            chrf_scores = [row_data[f'{d}_chrF++'] for d in source_directions]\n",
    "            \n",
    "            row_data[f'avg_from_{source_level}_BLEU'] = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "            row_data[f'avg_from_{source_level}_chrF++'] = np.mean(chrf_scores) if chrf_scores else 0.0\n",
    "\n",
    "        row_data['avg_overall_BLEU'] = np.mean([row_data[f'{d}_BLEU'] for d in directions])\n",
    "        row_data['avg_overall_chrF++'] = np.mean([row_data[f'{d}_chrF++'] for d in directions])\n",
    "        \n",
    "        results_list.append(row_data)\n",
    "\n",
    "    df = pd.DataFrame(results_list)\n",
    "\n",
    "    first_level = []\n",
    "    second_level = []\n",
    "\n",
    "    for direction in directions:\n",
    "        first_level.extend([direction, direction])\n",
    "        second_level.extend(['BLEU', 'chrF++'])\n",
    "\n",
    "    for source_level in range(4):\n",
    "        first_level.extend([f'avg_from_{source_level}', f'avg_from_{source_level}'])\n",
    "        second_level.extend(['BLEU', 'chrF++'])\n",
    "\n",
    "    first_level.extend(['avg_overall', 'avg_overall'])\n",
    "    second_level.extend(['BLEU', 'chrF++'])\n",
    "\n",
    "    columns = pd.MultiIndex.from_arrays([\n",
    "        ['Model'] + first_level,\n",
    "        [''] + second_level\n",
    "    ])\n",
    "\n",
    "    df_columns = ['Model']\n",
    "    for direction in directions:\n",
    "        df_columns.extend([f'{direction}_BLEU', f'{direction}_chrF++'])\n",
    "    for source_level in range(4):\n",
    "        df_columns.extend([f'avg_from_{source_level}_BLEU', f'avg_from_{source_level}_chrF++'])\n",
    "    df_columns.extend(['avg_overall_BLEU', 'avg_overall_chrF++'])\n",
    "    \n",
    "    df = df[df_columns]\n",
    "\n",
    "    df.columns = columns\n",
    "\n",
    "    numeric_columns = df.columns[1:]\n",
    "    df[numeric_columns] = df[numeric_columns].round(2)\n",
    "    \n",
    "    return df\n",
    "\n",
    "models_dict = {\n",
    "    'Model1': (model1, tokenizer1),\n",
    "    'Model2': (model2, tokenizer2),\n",
    "    'Model3': (model3, tokenizer3),\n",
    "    'Model4': (model4, tokenizer4),\n",
    "    'Model5': (model5, tokenizer5),\n",
    "    'Model6': (model6, tokenizer6),\n",
    "    'OpenAI': (ModelOpenAI, None),\n",
    "    'Gemini': (ModelGemini, None)\n",
    "}\n",
    "\n",
    "results_df = evaluate_all_models(models_dict, dataset)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_csv('../../results/task2/style_translation_results_<YOUR MODEL>.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6525db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
